{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSQIP - Stepwise Regression\n",
    "---\n",
    "\n",
    "Regressing on all of our predictors at once will tend to produce an over-fitted model. A traditional approach to this is to use a stepwise selection procedure, fitting individual best-fitting terms in sequence and adding them to build up a model in an interative fashion. These models are pretty common in the literature, but statisticians tend not to like them for a variety of reasons. Some of these are technical, but a pretty basic reason is that doing dozens of repeated statistical tests makes the meaning of each statistical test less valid.\n",
    "\n",
    "Regardless, we'll fit a stepwise logistic regression model here. To be clear, our model is of the form: \n",
    "\n",
    "$$P(y|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\sum_{i}\\beta_i x_i)}} $$\n",
    "\n",
    "... where $x$ is a vector of predictors, and $\\beta$ is a vector of coefficients. For variable selection at each step, we'll take the new model (produced by either adding a variable or dropping one) that provides the biggest reduction of the [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion). We stop making changes when no addition or removal of a variable reduces the AIC.\n",
    "\n",
    "We'll try to apply some of the modern bootstrapping technology here to define confidence intervals and estimate model performance. Our strategy will be\n",
    " - Divide the data into training and testing sets.\n",
    " - Fit a Stepwise Regression to the training set.\n",
    " - Use a bootstrap procedure to determine confidence intervals for the coefficients.\n",
    " - Check performance on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with imports of key libraries, and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Imputer\n",
    "from stepwiseLogisticRegression import stepwiseLogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn.metrics as sklm\n",
    "import random\n",
    "import warnings \n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\") # Ignore annoying warnings\n",
    "\n",
    "dataDir = './Data/'\n",
    "mungedFileName = dataDir + 'mungedData.pkl'\n",
    "\n",
    "cdf = pd.read_pickle(mungedFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine which response and predictor columns we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y is True for bad things happening.\n",
    "# 'readmission', 'reoperation', 'morbidity', 'mortality', 'infection'\n",
    "from outcomeVariables import getOutcomeVariable\n",
    "yFull = getOutcomeVariable(cdf, 'morbidity')\n",
    "\n",
    "# Drop rows with NaN outcome y data\n",
    "nanIdx = np.isnan(yFull).nonzero()\n",
    "yFull = np.delete(yFull.ravel(), nanIdx ,axis=0).reshape(-1,1)\n",
    "cdf.drop(cdf.index[nanIdx], axis=0, inplace=True)\n",
    "\n",
    "# Add a BMI column\n",
    "cdf.loc[:,'BMI'] = cdf.loc[:,'WEIGHT']*(1/2.20462)/(cdf.loc[:,'HEIGHT']*(1/39.3701)*\\\n",
    "                                                    cdf.loc[:,'HEIGHT']*(1/39.3701))\n",
    "\n",
    "# List of predictors to keep\n",
    "keepList1 = ['SEX', 'RACE_NEW','ETHNICITY_HISPANIC','AGE','ANESTHES','HEIGHT','WEIGHT','BMI',\\\n",
    "             'DIABETES','SMOKE','PACKS','ETOH','DYSPNEA','FNSTATUS2','VENTILAT',\\\n",
    "             'HXCOPD','CPNEUMON','HXCHF','HXMI','PRVPCI','PRVPCS','HXANGINA','HYPERMED',\\\n",
    "             'HXPVD','RENAFAIL','DIALYSIS','CVA','DISCANCR','WNDINF-','STEROID','WTLOSS',\\\n",
    "             'BLEEDDIS','PROPER30','ASACLASS','FNSTATUS1','RBC']\n",
    "# An additional list of predictors that might be interesting\n",
    "keepList2 = ['ASCITES','PRSODM','PRBUN','PRCREAT','PRALBUM','PRBILI','PRSGOT','PRALKPH',\\\n",
    "             'PRWBC','PRHCT','PRPLATE','PRPTT','PRINR','PRPT','PGY','ESOVAR','RESTPAIN',\\\n",
    "             'HXTIA','TRANSFUS','CHEMO','RADIO','PRSEPSIS','PREGNANCY','EMERGNCY',\\\n",
    "             'OPTIME','MALLAMP']\n",
    "\n",
    "# Combine the lists\n",
    "keepList = keepList1 + keepList2\n",
    "# Make a list of any column in cdf whose name starts with a string in keepList\n",
    "colsToKeep = [colName for colName in cdf.columns \\\n",
    "              if np.any([colName.startswith(keepItem) for keepItem in keepList])]\n",
    "cdf = cdf[colsToKeep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop predictors that don't have any variance. (This can happen because we already dropped a bunch of cases for which our outcomes are not recorded.)\n",
    "\n",
    "These regression models will fail on missing data, so we'll impute missing data for each column by setting it equal to the mean.\n",
    "\n",
    "We need to add an explicit intercept term.\n",
    "\n",
    "There's no regularization in this model, so there's no need to scale coefficients. This has the benefit of leaving us with interpretable coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Protect against only having one value in the columns.\n",
    "# (Imputer will drop these columns, and then we won't know the names...)\n",
    "for colName in cdf.columns:\n",
    "    colData = cdf[colName]\n",
    "    uniques = np.unique(colData)\n",
    "    nonNanUniques = [item for item in uniques if ~np.isnan(item)]\n",
    "    if len(nonNanUniques) < 2:\n",
    "        print('Not enough variance, dropping column: %s' % colName)\n",
    "        cdf.drop(colName, axis=1, inplace=True)\n",
    "\n",
    "# Impute missing data in cdf\n",
    "colNames = cdf.columns\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=True)\n",
    "imp.fit(cdf)\n",
    "X = imp.transform(cdf)\n",
    "X = pd.DataFrame(X, columns=colNames)\n",
    "\n",
    "# Add an intercept term.\n",
    "X['intercept'] = 1.0\n",
    "\n",
    "# Scale the columns\n",
    "# X = StandardScaler().fit_transform(cdf)\n",
    "# scaledX = pd.DataFrame(X, columns=colNames)\n",
    "scaledX = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break the data into separate fractions for training (2/3) and testing (1/3). All of our fitting and cross-validation will be done on the training set. The testing set will be held out to evaluate the final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split for validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaledX, yFull, test_size=0.33)\n",
    "y_train = y_train.reshape(-1,)\n",
    "y_test = y_test.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll create a stepwise regression object. This is unregularized. If we try to add a predictor to the model that's not linearly independent of other predictors, the regression will fail because the covariance matrix is singular. We just discard these as potential solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseAIC = 4726.272\n",
      "\t Adding: AIC = 4435.22 \t PRHCT\n",
      "baseAIC = 4435.221\n",
      "\t Adding: AIC = 4315.96 \t WNDINF-Yes\n",
      "baseAIC = 4315.964\n",
      "\t Adding: AIC = 4234.62 \t AGE\n",
      "baseAIC = 4234.622\n",
      "\t Adding: AIC = 4172.28 \t VENTILAT-Yes\n",
      "baseAIC = 4172.277\n",
      "\t Adding: AIC = 4134.19 \t DIABETES-INSULIN\n",
      "baseAIC = 4134.189\n",
      "\t Adding: AIC = 4100.03 \t OPTIME\n",
      "baseAIC = 4100.073\n",
      "\t Adding: AIC = 4075.31 \t BLEEDDIS-Yes\n",
      "baseAIC = 4075.312\n",
      "\t Adding: AIC = 4062.35 \t PRPTT\n",
      "baseAIC = 4062.352\n",
      "\t Adding: AIC = 4050.06 \t HXCOPD-Yes\n",
      "baseAIC = 4050.266\n",
      "\t Adding: AIC = 4038.48 \t PRPLATE\n",
      "baseAIC = 4038.480\n",
      "\t Adding: AIC = 4024.02 \t PRWBC\n",
      "baseAIC = 4024.034\n",
      "\t Adding: AIC = 4015.79 \t PRVPCI-Yes\n",
      "baseAIC = 4015.822\n",
      "\t Adding: AIC = 4009.65 \t PRALBUM\n",
      "baseAIC = 4008.076\n",
      "\t Adding: AIC = 4002.65 \t CVA-Yes\n",
      "baseAIC = 4004.581\n",
      "\t Adding: AIC = 3997.52 \t HYPERMED-Yes\n",
      "baseAIC = 3999.191\n",
      "\t Adding: AIC = 3993.64 \t PRALKPH\n",
      "baseAIC = 3994.852\n",
      "\t Adding: AIC = 3989.31 \t SMOKE-Yes\n",
      "baseAIC = 3992.318\n",
      "\t Adding: AIC = 3984.42 \t FNSTATUS1-Totally Dependent\n",
      "baseAIC = 3984.066\n",
      "\t Adding: AIC = 3980.46 \t CVANO-Yes\n",
      "baseAIC = 3981.438\n",
      "\t Adding: AIC = 3975.60 \t PRBUN\n",
      "baseAIC = 3975.904\n",
      "\t Adding: AIC = 3971.40 \t FNSTATUS1-Partially Dependent\n",
      "baseAIC = 3971.051\n",
      "\t Adding: AIC = 3966.71 \t RACE_NEW-American Indian or Alaska Native\n",
      "baseAIC = 3967.627\n",
      "\t Removing: AIC = 3964.22 \t intercept\n",
      "baseAIC = 3964.215\n",
      "\t Adding: AIC = 3962.32 \t PRBILI\n",
      "baseAIC = 3973.668\n",
      "\t Adding: AIC = 3960.63 \t ETHNICITY_HISPANIC-No\n",
      "baseAIC = 3961.038\n",
      "\t Adding: AIC = 3959.31 \t ANESTHES-MAC/IV Sedation\n",
      "baseAIC = 3958.547\n",
      "\t Adding: AIC = 3956.30 \t TRANSFUS-Yes\n",
      "baseAIC = 3958.303\n",
      "\t Adding: AIC = 3954.43 \t HXANGINA-Yes\n",
      "baseAIC = 3955.694\n",
      "\t Adding: AIC = 3952.24 \t DYSPNEA-No\n",
      "baseAIC = 3952.204\n",
      "\t Adding: AIC = 3950.56 \t DISCANCR-Yes\n",
      "baseAIC = 3949.825\n",
      "\t Adding: AIC = 3948.29 \t PROPER30-Yes\n",
      "baseAIC = 3948.330\n",
      "\t Adding: AIC = 3945.72 \t DYSPNEA-MODERATE EXERTION\n",
      "baseAIC = 3945.949\n",
      "\t Adding: AIC = 3945.44 \t HXMI-Yes\n",
      "baseAIC = 3950.672\n",
      "\t Adding: AIC = 3944.40 \t HXCHF-Yes\n",
      "baseAIC = 3945.548\n"
     ]
    }
   ],
   "source": [
    "# Do stepwise logistic regression, based on AIC\n",
    "swl = stepwiseLogisticRegression()\n",
    "\n",
    "# Bootstrap this whole regression\n",
    "nBoots = 2\n",
    "nSamp = X_train.shape[0]\n",
    "nPred = X_train.shape[1]\n",
    "bootParams = np.empty((nBoots, nPred))\n",
    "bootNParams = np.empty((nBoots,1))\n",
    "# For each bootstrap sample\n",
    "for bootN in np.arange(nBoots):\n",
    "    # Draw a collection of observations from the training set with replacement\n",
    "    sampIdx = [random.randint(0,nSamp-1) for n in np.arange(nSamp)]\n",
    "    y_boot = y_train[sampIdx].reshape(-1,)\n",
    "    X_boot = X_train.iloc[sampIdx,:]\n",
    "    X_boot = pd.DataFrame(X_boot, columns=X.columns)\n",
    "    \n",
    "    # No need to scale the columns\n",
    "    # X_boot = StandardScaler().fit_transform(X_boot)\n",
    "    # X_boot = pd.DataFrame(X_boot, columns=colNames)\n",
    "    \n",
    "    # Fit the model to the bootstrap sample. \n",
    "    swl = swl.fit(X_boot, y_boot, disp=False)\n",
    "    \n",
    "    # Store the coefficients (and the # of predictors) for each bootstrapped sample\n",
    "    coeffList = pd.Series(0.0,index=X.columns)\n",
    "    for index, colName in enumerate(swl.useCols):\n",
    "        coeffList[colName] = swl.model.coef_[0,index]\n",
    "    bootParams[bootN,:] = coeffList.values\n",
    "    bootNParams[bootN] = sum(coeffList.values != 0)\n",
    "    print('Booted #%d/%d with Nparams: %.3f' % (bootN+1, nBoots, bootNParams[bootN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we've bootstrapped, we can fit the model to the actual training data sample\n",
    "bestEst = swl.fit(X_train, y_train, disp=False)\n",
    "coeffList = pd.Series(0.0,index=X.columns)\n",
    "for index, colName in enumerate(bestEst.useCols):\n",
    "        coeffList[colName] = bestEst.model.coef_[0,index]\n",
    "coeffs = coeffList.values\n",
    "nParams = sum(coeffList.values != 0)\n",
    "print('N params: %d' % nParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify confidence intervals from the parameters fit to the bootstrapped samples\n",
    "lciIdx = round(.025*(nBoots-1))\n",
    "uciIdx = round(.975*(nBoots-1))\n",
    "bootParams.shape\n",
    "bootParams.sort(axis=0)\n",
    "lowCI = bootParams[lciIdx,:]\n",
    "upCI  = bootParams[uciIdx,:]\n",
    "allCols = [(coeffs[index], lowCI[index], upCI[index], colName) for index, colName\\\n",
    "           in enumerate(X_boot.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print out significant coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the coefficient if it was included in the final model\n",
    "# ... or if it was significant in the bootstrap.\n",
    "print('--- Terms in final model, and significant ---')\n",
    "for item in allCols:\n",
    "    if (item[0] != 0) and ((item[1] > 0) or (item[2] < 0)):\n",
    "        print('Coeff: %+.3f [%+.3f, %+.3f] %s  ' % item)\n",
    "print('--- Terms in final model, and NOT significant ---')\n",
    "for item in allCols:\n",
    "    if (item[0] != 0) and ~((item[1] > 0) or (item[2] < 0)):\n",
    "        print('Coeff: %+.3f [%+.3f, %+.3f] %s  ' % item)\n",
    "print('--- Terms NOT in final model, and significant ---')\n",
    "for item in allCols:\n",
    "    if (item[0] == 0) and ((item[1] > 0) or (item[2] < 0)):\n",
    "        print('Coeff: %+.3f [%+.3f, %+.3f] %s  ' % item)\n",
    "print('--- Terms NOT in final model, and NOT significant ---')\n",
    "for item in allCols:\n",
    "    if (item[0] == 0) and ~((item[1] > 0) or (item[2] < 0)):\n",
    "        print('Coeff: %+.3f [%+.3f, %+.3f] %s  ' % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curves for in-sample training data, and test data. The test data here was not used to fit the model at all, so it should approximate how our model will perform prospectively on new data. Furthermore, if we've done a good job regularizing, the in-sample performance should be pretty close to the test data performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot an ROC curve for training\n",
    "plt.clf\n",
    "pred_train = swl.predict_proba(X_train)\n",
    "rocAUC_train = sklm.roc_auc_score(y_train,pred_train)\n",
    "print('In-sample ROC AUC = %.3f ' % rocAUC_train)\n",
    "fpr, tpr, _ = sklm.roc_curve(y_train, pred_train)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr,label='In-sample')\n",
    "\n",
    "# Plot an ROC curve for test data\n",
    "pred_test = swl.predict_proba(X_test)\n",
    "rocAUC_test = sklm.roc_auc_score(y_test,pred_test)\n",
    "print('Test ROC AUC = %.3f ' % rocAUC_test)\n",
    "fpr, tpr, _ = sklm.roc_curve(y_test, pred_test)\n",
    "plt.plot(fpr,tpr,label='Test')\n",
    "\n",
    "# Format the ROC plot\n",
    "plt.plot([0, 1], [0, 1], color='k', linestyle=':')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this case you can see here how poorly this model generalizes to new test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
